{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up an environment using OpenAI Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('Taxi-v3') #creates and returns an environment object\n",
    "state = env.reset() #resets the environment's state and returns the state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "474\n"
     ]
    }
   ],
   "source": [
    "print(state) #states are represented as number between 0-499, you can display that number using this command\n",
    "#try resetting the environment to verify that it starts in a different state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.render() #you can \"see\" the current state through env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yellow = empty taxi\n",
    "Green = taxi with a passenger\n",
    "R, G, B, Y = four possible pick up/drop off locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(6)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space #aka state space\n",
    "env.action_space # action and observation space attributes are both type gym.spaces.discrete.Discrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions space Discrete(6)\n",
      "State space Discrete(500)\n"
     ]
    }
   ],
   "source": [
    "print(\"Actions space {}\".format(env.action_space))\n",
    "print(\"State space {}\".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "374 -1 False {'prob': 1.0}\n"
     ]
    }
   ],
   "source": [
    "observation, reward, done, info =  env.step(1) # take the action 1 = Go North. It returns (state, reward, done, info) \n",
    "env.render()\n",
    "print(observation, reward, done, info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a baseline agent that acts randomly and does not learn \n",
    "## (code name: nincompoop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomeAgent():\n",
    "    state = env.reset()\n",
    "    epoch = 0\n",
    "    reward = 0\n",
    "    while reward != 20:\n",
    "        observation, reward, done, info = env.step(env.action_space.sample())\n",
    "        epoch +=1\n",
    "#     env.render()\n",
    "    return epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'East'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_actions = {\n",
    "    0 : 'South',\n",
    "    1 : 'North',\n",
    "    2 : 'East',\n",
    "    3 : 'West',\n",
    "    4 : 'Pickup',\n",
    "    5 : 'Dropoff',\n",
    "}\n",
    "taxi_actions.get(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Q-Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplified Bellman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initateEnv():    #create environment\n",
    "    env = gym.make('Taxi-v3')\n",
    "\n",
    "    #initialize Q-table with all zeros\n",
    "    Q = np.zeros([env.observation_space.n, env.action_space.n]) \n",
    "\n",
    "    #set parameters\n",
    "    gamma = 0.1\n",
    "\n",
    "    #intialize reward\n",
    "    reward = 0\n",
    "\n",
    "    #initialize environment\n",
    "    state = env.reset()\n",
    "    \n",
    "    return env, Q , gamma, reward, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplifiedAgent():\n",
    "    env, Q , gamma, reward, state = initateEnv()\n",
    "    \n",
    "    epoch = 0\n",
    "    #create update loop\n",
    "    while reward != 20: #while dropoff state has not been reached\n",
    "        #choose current highest-valued action\n",
    "        action = np.argmax(Q[state])\n",
    "\n",
    "        #obtain reward and next state resulting from taking\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        #update Q-value for state-action pair\n",
    "        Q[state, action] = reward + gamma * np.max(Q[next_state])\n",
    "\n",
    "        #update state\n",
    "        state = next_state\n",
    "        \n",
    "        epoch += 1\n",
    "\n",
    "    #render final dropoff state\n",
    "#     env.render()\n",
    "#     print('Counter: {}'.format(count))\n",
    "    return epoch\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QlearningAgent(epsilon, alpha): \n",
    "    env, Q , gamma, reward, state = initateEnv()\n",
    "    epoch = 0 \n",
    "    while reward != 20:\n",
    "        if np.random.rand() < epsilon:\n",
    "            #exploration option\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            #exploitation option\n",
    "            action = np.argmax(Q[state])\n",
    "\n",
    "        #obtain reward and next state resulting from taking\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        #update Q-value for state-action pair\n",
    "        Q[state, action] = Q[state, action] + alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action])\n",
    "\n",
    "        #update state\n",
    "        state = next_state\n",
    "        epoch +=1\n",
    "\n",
    "    #render final dropoff state\n",
    "#     env.render()\n",
    "#     print('Counter: {}'.format(count))\n",
    "    return epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2683.02\n",
      "555.88\n",
      "722.64\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "score= []\n",
    "score2= []\n",
    "score3= []\n",
    "for _ in range(100):\n",
    "    score.append(randomeAgent())\n",
    "    score2.append(simplifiedAgent())\n",
    "    score3.append(QlearningAgent(0.1, 0.3))\n",
    "print(numpy.average(score))\n",
    "print(numpy.average(score2))\n",
    "print(numpy.average(score3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2539.23\n",
      "2451.327\n"
     ]
    }
   ],
   "source": [
    "s_randomA100 = []\n",
    "for _ in range(100):\n",
    "    s_randomA100.append(randomeAgent())\n",
    "print(numpy.average(s_randomA100))\n",
    "\n",
    "s_randomA1000 = []\n",
    "for _ in range(1000):\n",
    "    s_randomA1000.append(randomeAgent())\n",
    "print(numpy.average(s_randomA1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "671.14\n"
     ]
    }
   ],
   "source": [
    "s_QA = []\n",
    "for _ in range(100):\n",
    "    s_QA.append(QlearningAgent(0.1, 0.3))\n",
    "print(numpy.average(s_QA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{100: 738.9, 1000: 683.599}\n"
     ]
    }
   ],
   "source": [
    "result={}\n",
    "for eps in [100, 1000, 10000]:\n",
    "    s_QA = []\n",
    "    for _ in range(eps):\n",
    "        s_QA.append(QlearningAgent(0.1, 0.3))\n",
    "    result[eps] = numpy.average(s_QA)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1101, 1: 333, 2: 485, 3: 381, 4: 549, 5: 1366, 6: 849, 7: 1999, 8: 1277, 9: 319, 10: 1204, 11: 1081, 12: 1056, 13: 632, 14: 927, 15: 601, 16: 484, 17: 855, 18: 756, 19: 948, 20: 987, 21: 691, 22: 626, 23: 667, 24: 778, 25: 695, 26: 711, 27: 1302, 28: 642, 29: 1471, 30: 545, 31: 989, 32: 886, 33: 1156, 34: 1738, 35: 501, 36: 904, 37: 957, 38: 360, 39: 700, 40: 1717, 41: 645, 42: 933, 43: 1200, 44: 822, 45: 831, 46: 1367, 47: 1152, 48: 1311, 49: 905, 50: 800, 51: 1185, 52: 483, 53: 551, 54: 1084, 55: 1249, 56: 853, 57: 601, 58: 1182, 59: 868, 60: 620, 61: 484, 62: 357, 63: 733, 64: 668, 65: 1356, 66: 587, 67: 665, 68: 575, 69: 928, 70: 281, 71: 627, 72: 1057, 73: 524, 74: 478, 75: 894, 76: 219, 77: 960, 78: 743, 79: 1085, 80: 302, 81: 533, 82: 600, 83: 715, 84: 401, 85: 265, 86: 710, 87: 489, 88: 413, 89: 756, 90: 836, 91: 481, 92: 228, 93: 439, 94: 385, 95: 571, 96: 709, 97: 747, 98: 502, 99: 908}\n"
     ]
    }
   ],
   "source": [
    "env, Q , gamma, reward, state = initateEnv()\n",
    "\n",
    "gamma = 0.1\n",
    "alpha = 0.1\n",
    "epsilon = 0.1\n",
    "epsilon_decay = 0.99 #decay factor\n",
    "\n",
    "total_epochs = 0\n",
    "episodes = 100\n",
    "\n",
    "eps = {}\n",
    "for episode in range(episodes):\n",
    "    epochs = 0\n",
    "    reward = 0\n",
    "    epsilon = epsilon * epsilon_decay #decay step\n",
    "    state = env.reset()\n",
    "    \n",
    "    while reward != 20:\n",
    "        if np.random.rand() < epsilon:\n",
    "            #exploration option\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            #exploitation option\n",
    "            action = np.argmax(Q[state])\n",
    "\n",
    "        #obtain reward and next state resulting from taking\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        #update Q-value for state-action pair\n",
    "        Q[state, action] = Q[state, action] + alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action])\n",
    "\n",
    "        #update state\n",
    "        state = next_state\n",
    "        epochs +=1\n",
    "    eps[episode] = epochs\n",
    "\n",
    "print(eps) \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
